---
title: "Predicting Wine Quality"
author: "Brandon Wallace"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

This project predicts wine quality based a variety of wine properties. This markdown file contains the analysis process from start to finish. 

### Findings 

This report found that a random forest model performed best. The random forest model was able to predict whether or not a wine was of good quality or of bad quality with only a 0.165 misclassification rate on unseen test data. The most important predictors of quality were volatile acidity and percentage of the volume of alcohol. 

### Dataset 

This dataset contains 6,497 observations. Each observation is a wine. There are 11 predictor variables. The response variable is a class - either "Good" or "Bad" wine. 

```{r}
wineQuality <- read.csv(######## file path #########)

summary(wineQuality)

dim(wineQuality)
```

### Exploratory Data Analysis

I removed two outlier data points. One in sugar which was more than twice the size than the next largest value and one in free.sd which was also nearly twice the size of the next largest value. There is little evidence of multicollinearity. There is some strong relation between free.sd and total.sd since these are both measures of sulfur dioxide. 

```{r}
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
distribution_check <- wineQuality  %>% select(-label) %>% gather()

ggplot(data = distribution_check, mapping=aes(value)) +
  geom_histogram(color='blue', bins = 50) +
  facet_wrap(~key, scales = 'free_x') 
 
```


```{r}

head(sort(wineQuality$sugar,decreasing=TRUE),n=10)
head(sort(wineQuality$free.sd,decreasing=TRUE),n=10)
head(sort(wineQuality$total.sd,decreasing=TRUE),n=10)

```


```{r}
df <- subset(wineQuality, wineQuality$sugar != 65.80) 
```

```{r}
df <- subset(df, df$free.sd != 289.0)
```


```{r}

ggplot(df, aes(x = sugar, fill = "red")) +
         geom_histogram(bins =50, show.legend = FALSE) 

ggplot(df, aes(x = free.sd, fill = "red")) +
         geom_histogram(bins =50, show.legend = FALSE) 


```


```{r}
suppressWarnings(suppressPackageStartupMessages(library(corrplot)))

df %>% select(-label) %>%
  cor(.) %>%
  corrplot(.,method="ellipse")

```

### Data Splitting

I split the data into predictor variables and a response variable. I then split the training data and a test data by taking a random sample of the indexes of every row in the dataframe. I used a 70/30 split. I then used the indexes to save my four new variables.  

```{r}

set.seed(112)

resp <- df[,12]
pred <- df[,-12]

s <- sample(nrow(df),round(0.7*nrow(df)))

pred.train <- pred[s,]
pred.test <- pred[-s,]

resp.train <- resp[s]
resp.test <- resp[-s]

```


# Models 

### Best GLM Logistic Regression

The Logistic Regression model averaged a misclassification rate of 0.263. In order to make a prediction on the test data, I used a class separation threshold of 0.63 because there is a class imbalance in the response variable. 63% of the label column data are good wines, which represent Class 1 probabilities. This imbalance would systemically be pulled upwards toward 1, so the threshold is adjusted. The area under the curve for the ROC is 0.742. 


```{r} 
suppressWarnings(suppressPackageStartupMessages(library(bestglm)))

log.out = glm(resp.train~., data = pred.train, family = binomial)
summary(log.out)

```

```{r}
nrow(filter(df, label == "GOOD"))
nrow(filter(df, label == "BAD"))

nrow(filter(df, label == "GOOD")) / (nrow(filter(df, label == "GOOD")) + nrow(filter(df, label == "BAD")))
```


```{r}
log.prob = predict(log.out, newdata = pred.test, type = "response")
log.pred = ifelse(log.prob > 0.63, "GOOD", "BAD")

t<- table(log.pred,resp.test)
t
cat("The misclassification rate for the logistic model is ",round((t[1,2]+t[2,1])/sum(t),3),"\n")
cat("A sample of class 1 probilities are: ", predict(log.out, newdata = pred.test, type = "response")[1:5])

```

```{r}
suppressWarnings(suppressPackageStartupMessages(library(pROC)))

resp.test.numeric <- ifelse(resp.test == "GOOD", 1, 0)
log.pred.numeric <- ifelse(log.pred == "GOOD", 1, 0)

roc.out <- roc(resp.test.numeric ~ log.pred.numeric ,plot = TRUE, print.auc = TRUE)

cat("AUC for logistic model: ",round(roc.out$auc,3),"\n")

```


### Classification Tree 

The classification tree model performed slightly worse. The model produced a misclassifcation rate of 0.256. The area under the curve for the ROC was 0.706. The model is also using few splits with specific attention paid to volatile acidity and percentage of the volume of alcohol. Pruning did not improve performance on the test data. 

```{r}
suppressWarnings(suppressPackageStartupMessages(library(rpart)))

rp.out2 <- rpart(resp.train~.,data=pred.train)

success_pred <- predict(rp.out2, newdata = pred.test, type='class')

t2 <- table(success_pred, resp.test)
t2
cat("The misclassification rate for the classification tree model is ",round((t2[1,2]+t2[2,1])/sum(t2),3),"\n")
cat("A sample of class 1 probilities are: ", predict(rp.out2, newdata = pred.test, type='class')[1:5])

```

```{r}
plotcp(rp.out2)
```

```{r}
rpart.pruned <- prune(rp.out2,cp=0.038)
succcess_pred2 <- predict(rpart.pruned, newdata=pred.test, type="class")
t3 <- table(succcess_pred2, resp.test)
t3
cat("The misclassification rate for the pruned tree model is ",round((t3[1,2]+t3[2,1])/sum(t3),3),"\n")
cat("A sample of class 1 probilities are: ", predict(rpart.pruned, newdata = pred.test, type='class')[1:5])

```


```{r}
suppressWarnings(suppressPackageStartupMessages(library(rpart.plot)))
resp.test.numeric <- ifelse(resp.test == "GOOD", 1, 0)
success_pred_numeric <- ifelse(success_pred == "GOOD", 1, 0)

roc.out2 <- roc(resp.test.numeric ~ success_pred_numeric ,plot = TRUE, print.auc = TRUE)
cat("AUC for classification tree model: ",round(roc.out2$auc,3),"\n")

rpart.plot(rp.out2)
```

### Random Forest

The Random Forest model proved most effective. It produced a misclassification rate of 0.165- the lowest of the approaches in this report. The area under the curve on the ROC plot was 0.822. 


```{r}
set.seed(112)
suppressWarnings(suppressPackageStartupMessages(library(randomForest)))

rf.out <- randomForest(resp.train~., data = pred.train, importance = TRUE)

resp.pred <- predict(rf.out, newdata = pred.test, type = "prob")[,2]
resp.pred <- ifelse(resp.pred>0.5,"GOOD","BAD")

t4 <- table(resp.pred, resp.test)
t4
cat("The misclassification rate for the random forrest model is ",round((t4[1,2]+t4[2,1])/sum(t4),3),"\n")
cat("A sample of class 1 probilities are: ", predict(rf.out, newdata = pred.test, type = "prob")[1:5])

varImpPlot(rf.out,type=1)


```

```{r}
resp.pred.numeric <- ifelse(resp.pred == "GOOD", 1, 0)
resp.test.numeric <- ifelse(resp.test == "GOOD", 1, 0)

roc.out3 <- roc(resp.pred.numeric ~ resp.test.numeric ,plot = TRUE, print.auc = TRUE)
cat("AUC for random forest model is: ",round(roc.out3$auc,3),"\n")

```


### K Nearest Neighbor 

The KNN model found an optimal number of nearest neighbors to be 1- i.e. the best predictor of any given point's class was the point closest to it in space. The area under the curve in the ROC plot was 0.724. 

```{r}
suppressWarnings(suppressPackageStartupMessages(library(FNN)))

k.max = 50
misclas.k = rep(NA,k.max)
for ( kk in 1:k.max ) {
  knn.out = knn.cv(train = pred.train, cl = resp.train, k = kk, algorithm = "brute")
  misclas.k[kk] = mean(knn.out != resp.train)
}
k.min = which.min(misclas.k)
cat("The optimal number of nearest neighbors is ",k.min,"\n")

```

```{r}
ggplot(data=data.frame("k"=1:k.max,"misclass"=misclas.k),mapping=aes(x=k,y=misclass)) +
  geom_point() + geom_line() +
  xlab("Number of Nearest Neighbors k") + ylab("Validation Miclassification") +
  geom_vline(xintercept=k.min,color="red")
```


```{r}
knn.out = knn.cv(train=pred.train, cl=resp.train, k=k.min, algorithm="brute", prob = TRUE)
knn.pred = knn(train=pred.train,test=pred.test,cl=resp.train,k=k.min,algorithm="brute",prob=TRUE)
knn.prob = attributes(knn.pred)$prob
w = which(knn.pred=="BAD")
knn.prob[w] = 1 - knn.prob[w] 
```


```{r}
roc.out4 <- roc(resp.test,knn.prob, plot = TRUE, print.auc = TRUE)
cat("AUC for the KNN model is: ",round(roc.out4$auc,3),"\n")

```


### Support Vector Machine

The support vector machine model did not outperform the random forest model. The linear kernel for the SVM model outperformed the polynomial and the radial kernel with a misclassification rate of 0.248. The area under the curve for the best performing kernel was 0.712. 

```{r}
suppressWarnings(suppressPackageStartupMessages(library(e1071)))

set.seed(112)
training <- cbind(pred.train, resp.train)

tune.out = tune(svm,resp.train~., data=training, kernel="linear", ranges=list(cost=c(0.01, 0.1, 1 ,5 ,10)))

cat("The estimated optimal value for C is ",as.numeric(tune.out$best.parameters),"\n")

```


```{r}

best.resp.pred = predict(tune.out$best.model,newdata=pred.test)
mean(best.resp.pred!=resp.test) ; table(best.resp.pred,resp.test)

```


```{r}

tune.out = tune(svm,resp.train~., data=training, kernel="polynomial", ranges=list(cost=c(0.01, 0.1, 1 ,5 ,10)), degree = 2:4) 

cat("The estimated optimal values for C and degree are ",as.numeric(tune.out$best.parameters),"\n")

```


```{r}
resp.pred = predict(tune.out$best.model,newdata=pred.test)
mean(resp.pred!=resp.test) ; table(resp.pred,resp.test)

```


```{r}

tune.out = tune(svm,resp.train~., data=training, kernel="radial", ranges=list(cost=c(0.01, 0.1, 1 ,5 ,10)), gamma=10^seq(c(0.01, 0.1, 1 ,5 ,10)))

cat("The estimated optimal values for C and gamma are ",as.numeric(tune.out$best.parameters),"\n") 

```


```{r}

resp.pred = predict(tune.out$best.model,newdata=pred.test)
mean(resp.pred!=resp.test) ; table(resp.pred,resp.test)

```

```{r}
resp.pred.numeric <- ifelse(best.resp.pred == "GOOD", 1, 0)
resp.test.numeric <- ifelse(resp.test == "GOOD", 1, 0)
roc.out5 <- roc(resp.test.numeric,resp.pred.numeric, plot = TRUE, print.auc = TRUE)
cat("AUC for the KNN model is: ",round(roc.out5$auc,3),"\n")

```

## Optimized Model

The random forest model was the best performing model. In order to extract the most value from the random forest. I found the optimal class separation threshold using Youden's J Statistic. This value was 0.6114647. With this new class separation threshold, the misclassification rate was 0.165- the same as the previous model. 

```{r}

TP <- t4[2,2]
FP <- t4[2,1]
TN <- t4[1,1]
FN <- t4[1,2]

SENS <- TP / (TP + FP)
SPEC <- TN / (TN + FP)

YOUD_J <- (SENS + SPEC) - 1

YOUD_J

```

```{r}
rf.out <- randomForest(resp.train~., data = pred.train, importance = TRUE)

resp.pred <- predict(rf.out, newdata = pred.test, type = "prob")[,2]
resp.pred <- ifelse(resp.pred>YOUD_J,"GOOD","BAD")

t5 <- table(resp.pred, resp.test)
t5
cat("The misclassification rate for the new random forrest model is ",round((t4[1,2]+t4[2,1])/sum(t4),3),"\n")
cat("A sample of class 1 probilities are: ", predict(rf.out, newdata = pred.test, type = "prob")[1:5])

```






